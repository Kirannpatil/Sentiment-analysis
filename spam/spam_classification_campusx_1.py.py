# -*- coding: utf-8 -*-
"""spam classification campusx

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-uCvHaBCkQGXZpoeq2CPwXlFnp-BU2Hg
"""

pip install streamlit

import numpy as np
import pandas as pd

df= pd.read_csv('spam.csv', encoding='latin1')

df  # UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte
    # and so used encoding='latin1'

df.shape

df.info()

"""**last 3 columns has more than 95% missing  so we are dropping  those columns**"""

df.drop(columns=["Unnamed: 2","Unnamed: 3", "Unnamed: 4" ],inplace =True)

df.sample(5)

df.rename(columns= {'v1':'target','v2': 'text'},inplace =True)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target']= encoder.fit_transform(df['target'])

df.isnull().sum()

df.duplicated().sum()

df.duplicated()

duplicate = df[df.duplicated()]
duplicate.head(20)

df= df.drop_duplicates(keep='first')

df.shape

"""## EDA"""

df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(),labels= ['ham','spam'], autopct= '%0.2f')
plt.show()

"""**Data is imbalanced**"""

import nltk

nltk.download('punkt')

df['num_characters']= df['text'].apply(len)

df.head(10)

df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

df.head()

df['num_sentence'] =df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters', 'num_words','num_sentence']].describe()

df[df['target']==0][['num_characters', 'num_words','num_sentence']].describe()

df[df['target']==1][['num_characters', 'num_words','num_sentence']].describe()

import seaborn as sns

sns.histplot(df[df['target']==0]['num_characters'])
sns.histplot(df[df['target']==1]['num_characters'],color ='orange')

sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'],color ='magenta')

sns.pairplot(df,hue= 'target')

"""Preprocessing:
1. lower case
2. tokenization
3. remove special character
4. removing stop words and punctuation
5. stemming
"""

from nltk.corpus import stopwords
import string
import nltk
from nltk.stem import PorterStemmer
ps= PorterStemmer()

def transform(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y= []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()
    for i in text:
        y.append(ps.stem(i))

    return " ".join(y)

transform('did you like my presentation (20 on ml')

df['transformed_text'] = df['text'].apply(transform)

df['transformed_text']

from wordcloud import WordCloud  # this shows below box that shows most occuring spam words
wc = WordCloud(height =500, width = 500, min_font_size= 10, background_color= 'white')

spam_wc =wc.generate(df[df['target']== 1] ['transformed_text'].astype(str).str.cat(sep = " "))

plt.figure(figsize=(7,6))
plt.imshow(spam_wc)

"""**large size words are those which appear most in spam**"""

spamcorpus = []
for msg in df[df['target']==1]['transformed_text'].astype(str).tolist():
    for word in msg.split():
        spamcorpus.append(word)

len(spamcorpus)

from collections import Counter
spam_counter =Counter(spamcorpus).most_common(20)
spam_df = pd.DataFrame(spam_counter, columns = ['word','count'])

sns.barplot(x= 'word',y='count', data=spam_df)

plt.xticks(rotation=90)
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Top 20 most common words in spam messages')
plt.show()

"""## Model buliding"""

# text vectorization with BOW  as Naive bayes gives best result on textual data
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()

X =cv.fit_transform(df['transformed_text']).toarray()

X.shape

X

y= df['target'].values

y

from sklearn.model_selection import train_test_split

X_train, X_test,y_train,y_test = train_test_split(X,y,test_size= 0.2, random_state = 2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()

X =tfidf.fit_transform(df['transformed_text']).toarray()

X.shape

y= df['target'].values

X_train, X_test,y_train,y_test = train_test_split(X,y,test_size= 0.2, random_state = 2)

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

"""TP = 96 (spam emails correctly predicted as spam)

TN = 896 (ham emails correctly predicted as not spam)

FP = 0 (no ham emails incorrectly predicted as spam)

FN = 42 (spam emails incorrectly predicted as ham)
"""

bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

"""**Precision score matters most here as data is imbalanced so Multinomial is giving best result. With zero False positive**"""

from sklearn.svm import SVC
svc = SVC(kernel='sigmoid', gamma=1.0)

from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(max_depth=5)

from sklearn.linear_model import LogisticRegression
lrc = LogisticRegression(solver='liblinear', penalty='l1')

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import AdaBoostClassifier
abc = AdaBoostClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import BaggingClassifier
bc = BaggingClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import GradientBoostingClassifier
gbdt = GradientBoostingClassifier(n_estimators=50, random_state=2)

from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=50, random_state=2)

clfs = {
    'SVC': svc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT': gbdt,
    'xgb': xgb
}

def train_classifier(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    return accuracy, precision

train_classifier(svc,X_train, y_train, X_test, y_test)

from sklearn.metrics import accuracy_score, precision_score

accuracy_scores =[]
precision_scores = []

for name,clf in clfs.items():
    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)
performance_df

# model improve
# 1. Change the max_features parameter of TfIdf

tfidf = TfidfVectorizer(max_features = 3000) # it can be 500, 1000...etc. campus X found 3000 giving good result

X =tfidf.fit_transform(df['transformed_text']).toarray()

X.shape

y= df['target'].values

X_train, X_test,y_train,y_test = train_test_split(X,y,test_size= 0.2, random_state = 2)

mnb2 = MultinomialNB()

mnb2.fit(X_train,y_train)
y_pred2 = mnb2.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

"""**Here result of mnb much bettter with hyperparameter tuning of max features**"""

from sklearn.svm import SVC
svc = SVC(kernel='sigmoid', gamma=1.0)

from sklearn.naive_bayes import MultinomialNB
mnb2 = MultinomialNB()

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(max_depth=5)

from sklearn.linear_model import LogisticRegression
lrc = LogisticRegression(solver='liblinear', penalty='l1')

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import AdaBoostClassifier
abc = AdaBoostClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import BaggingClassifier
bc = BaggingClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)

from sklearn.ensemble import GradientBoostingClassifier
gbdt = GradientBoostingClassifier(n_estimators=50, random_state=2)

from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=50, random_state=2)

clfs = {
    'SVC': svc,
    'NB': mnb2,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT': gbdt,
    'xgb': xgb
}

accuracy_scores =[]
precision_scores = []

for name,clf in clfs.items():
    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)
new_df = performance_df.merge(temp_df,on='Algorithm')
new_df

# Voting Classifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
RF = RandomForestClassifier(n_estimators=50, random_state=2)
mnb = MultinomialNB()
from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(estimators=[('svm', svc),('rf', rfc), ('nb', mnb2), ],voting='soft')

voting.fit(X_train,y_train)

VotingClassifier(estimators=[('svm',
                              SVC(gamma=1.0, kernel='sigmoid',
                                  probability=True)),
                             ('nb', MultinomialNB()),
                             ('rf',
                              RandomForestClassifier(n_estimators=50,
                                                   random_state=2))],
                 voting='soft')

y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb2,open('model.pkl','wb'))

